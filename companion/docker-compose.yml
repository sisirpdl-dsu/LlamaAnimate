version: '3'
services:
  text_generator:
    image: ollama/ollama
    container_name: ollama
    ports:
      - "11434:11434"
        #deploy:
        #resources:
        #reservations:
        #devices:
        #    - driver: nvidia
        #      capabilities: ["gpu"]
        #      count: all  # Adjust count for the number of GPUs you want to use
    command: "serve && ollama run llama3.2"
  voice:
    container_name: piper
    image: piper
    ports:
      - "69:69"
    build: ./piper-master/piper-master
    tty: true
    privileged: true
    devices: 
      - "/dev/snd:/dev/snd"
    environment:
      - PULSE_SERVER=/mnt/wslg/PulseServer #unix:/run/user/$(id -u)/pulse/native
    volumes:
      - /etc/asound.conf:/etc/asound.conf # Respect the hosts configured default audio device
      - /mnt/wslg:/mnt/wslg
  master:
    image: master
    container_name: master
    build: .
    stdin_open: true   
    tty: true 
    command: /bin/bash install_ollama3.2 
    privileged: true
    devices: 
      - "/dev/snd:/dev/snd"
    environment:
      - /etc/machine-id:/etc/machine-id
    volumes:
      - /etc/asound.conf:/etc/asound.conf # Respect the hosts configured default audio device
      - /mnt/wslg:/mnt/wslg
    depends_on:
      text_generator:
        condition: service_started
      voice:
        condition: service_started
    network_mode: host


